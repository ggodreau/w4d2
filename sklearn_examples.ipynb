{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `train_test_split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data, take `TAX`, `CRIM`, and `PTRATIO` and assign to `X`. Assign target to `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_boston()\n",
    "\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "X = boston[['TAX', 'CRIM', 'PTRATIO']].copy()\n",
    "boston_y = pd.DataFrame(data.target)\n",
    "\n",
    "print(X.head())\n",
    "print(boston_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `X_train, X_test, y_train, y_test` using train-test split with a 33% holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(boston, boston_y, test_size=0.33)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a linear regression to the **training** set (`X_train` and `y_train`) and look at the $R^2$ score **on the training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the $R^2$ using the **training set** misleading?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without refitting the data**, look at the $R^2$ score on the **test set** (`X_test` and `y_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this a more accurate measure of how well our linear regression of `TAX`, `CRIM`, and `PTRATIO` on `MDEV`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `datasets/salary.csv` dataset. This dataset records the salary of academics and measures the following features:\n",
    "\n",
    "- sx = Sex, coded 1 for female and 0 for male\n",
    "- rk = Rank, coded\n",
    "  - 1 for assistant professor,\n",
    "  - 2 for associate professor, and\n",
    "  - 3 for full professor\n",
    "- yr = Number of years in current rank\n",
    "- dg = Highest degree, coded 1 if doctorate, 0 if masters\n",
    "- yd = Number of years since highest degree was earned\n",
    "- sl = Academic year salary, in dollars.\n",
    "\n",
    "Import the salary data, assign the `yr` and `yd` columns to a variable named `X`, then assign `sl` to a variable named `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `train_test_split()` to create a training set and a test set, split 50/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a linear regression using the training set. What does the $R^2$ look like for the training set? What does the $R^2$ look like for the test set? Which is higher? Which is a more accurate number? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `cross_val_score()` and `cross_val_predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `cross_val_score()` and `cross_val_predict()` to do cross-validation and to take a quick look at each fold.\n",
    "\n",
    "Using the boston data, let's pick `CRIM`, `ZN`, and `INDUS` for our predictors and assign them to `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston[['CRIM', 'ZN', 'INDUS']].copy()\n",
    "print(X.head())\n",
    "print(boston_y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `cross_val_score()` and five-fold cross-validation to see the $R^2$ for each of the folds compared to the test set for that fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "print(cross_val_score(lr, X, boston_y, cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the low $R^2$ scores mean? If they were there, what does a **negative** $R^2$ show?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also want to see an overall score for these folds. Typically, we will take the average $R^2$ across each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lr, X, boston_y, cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `cross_val_predict()` and assign the predicted values for each test fold to an object called `predictions`. How many predictions should we have in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(lr, X, boston_y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_val_predict()` lets us use the predictions from each fold to look at metrics that are important but not part of `LinearRegression()`'s `.score()` method, such as the mean squared error. The mean squared error is the average difference between the true y and the predicted y:\n",
    "\n",
    "$$ \\text{Mean Squared Error} = \\frac{\\sum(y_i - \\hat{y}_i)^2}{n} $$\n",
    "\n",
    "We will import sklearn's `mean_squared_error()` function which takes in a true y and a predicted y and outputs the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(boston_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Boston housing data above, try a different set of columns -- does the $R^2$ on the folds shown by `cross_val_score()` improve? What about the MSE using the predictions output by `cross_val_predict()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `KFold`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also lets us manually create folds, which we can then call other options on. This is helpful if we want to do more advanced work on each of those folds. \n",
    "\n",
    "In the following example, we will fit a linear regression using every column in the Boston housing data and validating its efficacy using 5-fold cross-validation. For each fold, I will calculate the $R^2$ and the MSE -- this would be difficult to do without using `KFolds` to manually split up the data.\n",
    "\n",
    "For ease of indexing, I am converting `X` into a `numpy` array using the `.values` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.copy().values\n",
    "boston_y = boston_y.values\n",
    "\n",
    "print(X.shape, boston_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a `KFold` object with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`KFold` will take in one or two data collections and return back a set of paired indices for the folds in question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for training_index, test_index in kfolds.split(X, boston_y):\n",
    "    print('training: ', training_index[0:5], '...', training_index[-6:-1], \n",
    "          '\\ntest: ', test_index[0:5], '...', test_index[-6:-1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use these indices to manually split up `X` and `boston_y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for training_index, test_index in kfolds.split(X, boston_y):\n",
    "    x_train = X[training_index]\n",
    "    x_test = X[test_index]\n",
    "    y_train = boston_y[training_index]\n",
    "    y_test = boston_y[test_index]\n",
    "    print('fold', counter, '-', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "    counter += 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then feed these folds into `LinearRegression()` to fit it to a given fold and then use `.score()` and `mean_squared_error()` to look at the fit on the test set in each fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "lr = LinearRegression()\n",
    "for training_index, test_index in kfolds.split(X, boston_y):\n",
    "    x_train = X[training_index]\n",
    "    x_test = X[test_index]\n",
    "    y_train = boston_y[training_index]\n",
    "    y_test = boston_y[test_index]\n",
    "    lr.fit(x_train, y_train)\n",
    "    r2_score = lr.score(x_test, y_test)\n",
    "    fold_predictions = lr.predict(x_test)\n",
    "    mse_score = mean_squared_error(y_test, fold_predictions)\n",
    "    print('Fold', counter, '- r^2 score:', r2_score, 'mse:', mse_score)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `KFold` we are able to access the folds _as they are created_ and can learn more about how well each fold does using different statistics. \n",
    "\n",
    "**Discussion Questions**:\n",
    "\n",
    "1. Why might we want to look at different goodness of fit metrics during the cross-validation process?\n",
    "2. It looks like our $R^2$ changes pretty frequently but our `mse` is not particularly volatile in this process. Have we made a good modeling choice by including every column in Boston? Why or why not? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
